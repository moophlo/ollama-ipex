apiVersion: v1
kind: Pod
metadata:
  name: intel-gpu
  namespace: llm
  labels:
    app: intel-gpu
spec:
  containers:  # Corrected indentation here
    - env:
        - name: OLLAMA_KEEP_ALIVE
          value: "60m"
        - name: DEFAULT_REQUEST_TIMEOUT
          value: "240"
        - name: OLLAMA_HOST
          value: "0.0.0.0:11435"
        - name: HOME
          value: "/home/ollama/models"
        - name: ZES_ENABLE_SYSMAN
          value: "1"
        - name: DEVICE
          value: "iGPU"
        - name: OLLAMA_INTEL_GPU
          value: "1"
        - name: GIN_MODE
          value: "release"
      name: intel-gpu
      image: moophlo/ollama-ipex:latest
      imagePullPolicy: IfNotPresent
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
            - SYS_TRACE
      volumeMounts:
        - mountPath: /root/.ollama
          name: ollama-data
        - mountPath: /home/ollama/models
          name: pv-ollama-home-ipex
      ports:
        - containerPort: 11435
          name: http
          protocol: TCP
      resources:
        limits:
          gpu.intel.com/i915: 1
  hostNetwork: true
  tolerations:
    - effect: NoSchedule
      key: gpu
      operator: Equal
      value: "true"
  volumes:
    - emptyDir: {}
      name: ollama-data
    - name: pv-ollama-home-ipex
      persistentVolumeClaim:
        claimName: pv-ollama-home-ipex

